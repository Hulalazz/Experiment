\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Determinant Gradient Method}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Gradient Descent}{1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{GD}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Momentum Methods}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient Descent with momentum\relax }}{2}}
\newlabel{momentum}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The Dynamics in Optimization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Some Examples}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}The Projected Gradient Methods}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Newton's Method for Quadratic Optimization}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Stochastic Gradient Descent}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Primary Stochastic Gradient Descent\relax }}{3}}
\newlabel{SGD}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Difference with Full Gradient Descent}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Adaptive Learning Rate}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1}Adagrad}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.2}RMSprop}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Adagrad Algorithm\relax }}{4}}
\newlabel{Adagrad}{{4}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces RMSprop Algorithm\relax }}{4}}
\newlabel{RMSprop}{{5}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.3}Adam}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Adam\relax }}{4}}
\newlabel{Adam}{{6}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Variance Reduction}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces SVRG\relax }}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.0.4}Stochastic Average Gradient}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}SVRG}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}SVRG++}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces SVRG++\relax }}{6}}
\newlabel{SVRG++}{{8}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}SCSD}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces SCSD\relax }}{6}}
\newlabel{SCSD}{{9}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}The Stochastic Properties}{6}}
