{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Basic-Idea\" data-toc-modified-id=\"Basic-Idea-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Basic Idea</a></div><div class=\"lev1 toc-item\"><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Line-serach\" data-toc-modified-id=\"Line-serach-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Line serach</a></div><div class=\"lev3 toc-item\"><a href=\"#Inexact-line-serach\" data-toc-modified-id=\"Inexact-line-serach-311\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Inexact line serach</a></div><div class=\"lev3 toc-item\"><a href=\"#Convergence-in-Convex-case\" data-toc-modified-id=\"Convergence-in-Convex-case-312\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Convergence in Convex case</a></div><div class=\"lev2 toc-item\"><a href=\"#Subgradient\" data-toc-modified-id=\"Subgradient-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Subgradient</a></div><div class=\"lev2 toc-item\"><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Momentum</a></div><div class=\"lev3 toc-item\"><a href=\"#Nesterov-Momentum\" data-toc-modified-id=\"Nesterov-Momentum-331\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Nesterov Momentum</a></div><div class=\"lev1 toc-item\"><a href=\"#More-on-Gradient-Descent\" data-toc-modified-id=\"More-on-Gradient-Descent-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>More on Gradient Descent</a></div><div class=\"lev1 toc-item\"><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Motivation</a></div><div class=\"lev2 toc-item\"><a href=\"#Why-SGD-?\" data-toc-modified-id=\"Why-SGD-?-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Why SGD ?</a></div><div class=\"lev2 toc-item\"><a href=\"#Adaptive-learning-rate\" data-toc-modified-id=\"Adaptive-learning-rate-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Adaptive learning rate</a></div><div class=\"lev3 toc-item\"><a href=\"#Adagrad\" data-toc-modified-id=\"Adagrad-531\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Adagrad</a></div><div class=\"lev3 toc-item\"><a href=\"#RMSprop\" data-toc-modified-id=\"RMSprop-532\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>RMSprop</a></div><div class=\"lev3 toc-item\"><a href=\"#Adam\" data-toc-modified-id=\"Adam-533\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Adam</a></div><div class=\"lev2 toc-item\"><a href=\"#Variance-reduction\" data-toc-modified-id=\"Variance-reduction-54\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Variance reduction</a></div><div class=\"lev3 toc-item\"><a href=\"#Stochastic-Average--Gradient-(SAG)\" data-toc-modified-id=\"Stochastic-Average--Gradient-(SAG)-541\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Stochastic Average  Gradient (SAG)</a></div><div class=\"lev3 toc-item\"><a href=\"#SVRG\" data-toc-modified-id=\"SVRG-542\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>SVRG</a></div><div class=\"lev3 toc-item\"><a href=\"#SVRG++\" data-toc-modified-id=\"SVRG++-543\"><span class=\"toc-item-num\">5.4.3&nbsp;&nbsp;</span>SVRG++</a></div><div class=\"lev3 toc-item\"><a href=\"#SAGA\" data-toc-modified-id=\"SAGA-544\"><span class=\"toc-item-num\">5.4.4&nbsp;&nbsp;</span>SAGA</a></div><div class=\"lev3 toc-item\"><a href=\"#SCSG\" data-toc-modified-id=\"SCSG-545\"><span class=\"toc-item-num\">5.4.5&nbsp;&nbsp;</span>SCSG</a></div><div class=\"lev2 toc-item\"><a href=\"#Summary:-the-3-directions-in-SGD\" data-toc-modified-id=\"Summary:-the-3-directions-in-SGD-55\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Summary: the <em>3</em> directions in SGD</a></div><div class=\"lev3 toc-item\"><a href=\"#How-to-choose-the-steps?\" data-toc-modified-id=\"How-to-choose-the-steps?-551\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>How to choose the steps?</a></div><div class=\"lev3 toc-item\"><a href=\"#How-to-reduce-the-variance-of-SGD?\" data-toc-modified-id=\"How-to-reduce-the-variance-of-SGD?-552\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>How to reduce the variance of SGD?</a></div><div class=\"lev3 toc-item\"><a href=\"#How-to-accelerate-SGD?\" data-toc-modified-id=\"How-to-accelerate-SGD?-553\"><span class=\"toc-item-num\">5.5.3&nbsp;&nbsp;</span>How to accelerate SGD?</a></div><div class=\"lev1 toc-item\"><a href=\"#The-Loss-Surface-of-Deep-Learning\" data-toc-modified-id=\"The-Loss-Surface-of-Deep-Learning-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>The Loss Surface of Deep Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#Fundamental-Deep-Learning-Problem\" data-toc-modified-id=\"Fundamental-Deep-Learning-Problem-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Fundamental Deep Learning Problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Regularization-=-Sparity?\" data-toc-modified-id=\"Regularization-=-Sparity?-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Regularization = Sparity?</a></div><div class=\"lev2 toc-item\"><a href=\"#Nonconvex-Optimization\" data-toc-modified-id=\"Nonconvex-Optimization-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Nonconvex Optimization</a></div><div class=\"lev3 toc-item\"><a href=\"#Convexify\" data-toc-modified-id=\"Convexify-631\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Convexify</a></div><div class=\"lev3 toc-item\"><a href=\"#Convex-splitting\" data-toc-modified-id=\"Convex-splitting-632\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>Convex splitting</a></div><div class=\"lev3 toc-item\"><a href=\"#Smoothing\" data-toc-modified-id=\"Smoothing-633\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>Smoothing</a></div><div class=\"lev1 toc-item\"><a href=\"#How-to-implement-it?\" data-toc-modified-id=\"How-to-implement-it?-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>How to implement it?</a></div><div class=\"lev2 toc-item\"><a href=\"#Read-the-code-in-TensorFlow\" data-toc-modified-id=\"Read-the-code-in-TensorFlow-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Read the code in TensorFlow</a></div><div class=\"lev2 toc-item\"><a href=\"#Mirror,-natural-or-zero-gradient?\" data-toc-modified-id=\"Mirror,-natural-or-zero-gradient?-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Mirror, natural or zero gradient?</a></div><div class=\"lev2 toc-item\"><a href=\"#Noise,-Perturbation-and-Initialization\" data-toc-modified-id=\"Noise,-Perturbation-and-Initialization-73\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Noise, Perturbation and Initialization</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "Stochastic gradient descent (SGD in short) is the popular optimization algoirthm in deep learning.\n",
    "It is easy to implement and of much advantages in differentiable optimization within high dimension.\n",
    "This tutorial is aimed to introduce some variants of SGD for my thesis.\n",
    "\n",
    "There are some related links:\n",
    "* [Some link on SGD](https://www.metacademy.org/graphs/concepts/stochastic_gradient_descent). \n",
    "* [Learning and optimization](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf).\n",
    "* [Backpropagation](http://timvieira.github.io/blog/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Idea\n",
    "Gradient descent or steepest descnet is the fundamental first order optimization only using the gradient of the object function.\n",
    "It is simple and especially efficent in convex optimization problem.\n",
    "But when the dimension of the objective function is too high, it is not easy to compute its gradient owinng to the dimension curse.\n",
    "To void that, we just grasp some \"local\" or \"partial\" information in the  low dimension subspace instead of the full information in high dimension space. \n",
    "\n",
    "***\n",
    "For example, the coordinate descent algorithm is to optimize the multivariable fnction  in one specific variable or coordinate while the rest are fixed.\n",
    "***\n",
    "It is natural for us to take the advantage of \"partial\" gradient information in place of the full gradient information with affordable computation cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "[See](https://www.wikiwand.com/en/Gradient_descent)\n",
    "Gradient descent is based on the local information -  it is  the negative gradinet that is most speedy to decrease for a function in a given point. Let $f(x)$ be the objective function and $\\nabla f(x) = g(x)$.\n",
    ">1. Give the initial value: $x_0$\n",
    ">2. Update the parameters with small step: $$x_{k+1}=x_{k} - \\alpha g(x_{k})$$\n",
    "\n",
    "*Note: $\\alpha$ must be small enough to ensure that it is descent: $f(x_{k+1})< f(x_{k})$.\n",
    "***\n",
    "The step is key factor in the convergence proof of the gradinet descent.\n",
    "One common setting is diminishing but not summable: $$\\sum_{k=0}^{\\infty} {\\alpha}_{k}= \\infty,\\sum_{k=0}^{\\infty} {\\alpha}_{k}^{2}< \\infty $$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line serach\n",
    "\n",
    "The ideal choice of $\\alpha$ is the global minimizer of $\\phi(\\alpha)=f(x_k-\\alpha g(x_k)), \\alpha > 0$.\n",
    "However, it is too expensive to compute it.\n",
    "\n",
    "### Inexact line serach\n",
    "\n",
    "The first condition is the step size should give the decrease in the objective function\n",
    " $f(x_k)< f(x_{k-1}) - c_1\\alpha g_k^2, \\alpha > 0, c_1 \\in (0,1)$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence in Convex case\n",
    "$$min_{x} f(x),$$where f is convex and $L$-*Lipschitz*.\n",
    "By gradient descent, we can obtain:$$f(\\sum_{i=1}^{K}x_{i}) - f(x^*) \\le \\frac{RL}{\\sqrt{K}}.$$\n",
    ">Proof: By the convexity,$$f(x_t)-f(x^*)\\le g_{t}^T(x_t - x^*).$$ By the iteration, $$g_{t}=\\frac{1}{\\alpha}(x_{t}-x_{t+1}).$$\n",
    "Combine these 2 relation, we get $$\\begin{align}\n",
    "                    f(x_t)-f(x^*)\n",
    "                    &\\le \\frac{1}{\\alpha}(g_t)^T (x_t - x^*) \\\\\n",
    "                    &= \\frac{1}{\\alpha}(x_{t}-x_{t+1})^T (x_t - x^*)\\\\\n",
    "                    &= \\frac{1}{2\\alpha}(\\| x_t -x^*\\|^2 + \\| x_t-x_{t+1}\\|^2 - \\| x_{t+1}-x^*\\|^2)\\\\\n",
    "                    &= \\frac{1}{2\\alpha}(\\| x_t -x^*\\|^2 - \\| x_{t+1}-x^*\\|^2)+\\frac{\\alpha}{2}\\|g_t\\|^2\\\\\n",
    "                    &\\le \\frac{1}{2\\alpha}(\\| x_t -x^*\\|^2 - \\| x_{t+1}-x^*\\|^2) + \\frac{\\alpha}{2} L^2 \\tag{By L-Lipschitz}\n",
    "                    \\end{align}.$$\n",
    "Sum over $t=1,2,\\cdots,K$, we obtain that:\n",
    "$$\\begin{align}\n",
    "\\sum _{t=1}^K (f(x_t)-f(x^*)) \\le \\frac{1}{2\\alpha} \\sum_{t=1}^K (\\| x_t -x^*\\|^2 - \\| x_{t+1}-x^*\\|^2) + \\frac{K\\alpha}{2} L^2\\\\\n",
    "= \\frac{1}{2\\alpha} (\\| x_1 -x^*\\|^2 - \\| x_{K+1}-x^*\\|^2)+ \\frac{K\\alpha}{2} L^2 \\\\\n",
    "\\le \\frac{R^2}{2}+\\frac{K\\alpha}{2} L^2\n",
    "\\end{align}.$$\n",
    "Then we have that:$$\\frac{1}{K}\\sum _{t=1}^K (f(x_t)-f(x^*)) \\le \\frac{R^2}{2K}+\\frac{\\alpha}{2} L^2.$$\n",
    "Replace the stepsize $\\alpha = \\frac{R}{L\\sqrt{K}}$, and then $$\\frac{1}{K}\\sum _{t=1}^K (f(x_t)-f(x^*)) \\le \\frac{RL}{\\sqrt{K}}.$$\n",
    "By Jension's inequality $ f(\\sum_{t=1}^K x_t)\\le \\frac{1}{k}\\sum _{t=1}^K (f(x_t)$, we conclude that $$ f(\\sum_{t=1}^K x_t)-f(x^*)) \\le \\frac{RL}{\\sqrt{K}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgradient\n",
    "Not all convex function is smooth or differential. The subgradient is related with the convexity and extend the conception *gradient*.\n",
    "In fact, the gradient can be replaced by subgradient and the proof and properties are simliar.[Subgradient in Wikipedia](https://www.wikiwand.com/en/Subgradient_method).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "Momentum method helps accelerate GD in the relevant direction and dampens oscillations. It introduces momentum instead of gradient  inspired by physics.\n",
    "***\n",
    "$$v_k= \\gamma  v_{k-1} -\\alpha g({\\theta}_k)$$\n",
    "$${\\theta}_{k+1}={\\theta}_k + v_k$$\n",
    "***\n",
    "The momentum term $\\gamma$ is usually set to 0.9 or a similar value.\n",
    "The momentum parameter $\\gamma$ is analogous to the mass of Newtonian particles that move through a viscous medium in a\n",
    "conservative force field([shown in here](https://web.archive.org/web/20140508200053/http://brahms.cpmc.columbia.edu/publications/momentum.pdf)). \n",
    "\n",
    "\n",
    "In other words,$${\\theta}_{k+1}={\\theta}_k -\\alpha \\sum _{j=1}^{k}{\\gamma}^{k-j}g({\\theta}_j) $$.\n",
    "If $\\gamma < 1$, we can see that the update is relative with the whole history but the correlation is decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum\n",
    "[Nesterov’s Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/pdf/1607.01981.pdf).\n",
    "\n",
    "In [Revisiting Nesterov's Acceleration](https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/), this momentum is studied from diffferent perspectives to reveal the acceleration [as well as this paper](https://distill.pub/2017/momentum/).\n",
    "\n",
    "***\n",
    "$$v_k= \\gamma  v_{k-1}-\\alpha g({\\theta}_k+v_{k-1})$$\n",
    "$${\\theta}_{k+1}={\\theta}_k + v_k$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Gradient Descent\n",
    "Take it as fixed point or Baysian inference?\n",
    "* [More paper](http://www.arxiv-sanity.com/search?q=gradient+descent).\n",
    "* [Momentum](https://arxiv.org/abs/1611.02635).\n",
    "* [A Variational Perspective on Accelerated Methods in Optimization](https://arxiv.org/abs/1603.04245).\n",
    "* [A fixed-point of view](https://arxiv.org/abs/1706.09880).\n",
    "* [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983v5).\n",
    "* [Approximate Bayesian Inference](https://arxiv.org/abs/1704.04289v1).\n",
    "* [Inconsistent Stochastic Gradient Descent](https://arxiv.org/abs/1603.05544v3).\n",
    "* [Improving Stochastic Gradient Descent with Feedback](https://arxiv.org/abs/1611.01505v2).\n",
    "* [A Cyclic Incremental Method](https://arxiv.org/abs/1611.00347v1).\n",
    "* [A blog on gradient descent and more](http://freemind.pluskid.org/machine-learning/gradient-descent-wolfe-s-condition-and-logistic-regression/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "*Stochastic* means that this method is not to compute the exact gradinet but to estimate the gradient by sampling.\n",
    "It is always applied to the optimization in the finite sum form. For example, it is to solve the mean square errror of least square method in large scale. \n",
    "\n",
    "The object function is called empirical risk function with the\n",
    "form $f(x_i;\\theta)=\\sum_{i=1}^{n}f_{i}(\\theta)$, where $x_i$ is constant given by training sample and $f_{i}$have the same independent variables $\\theta$. Let $\\nabla f_{i}(x) = g_{i}(x)$. The stochastic gradient descent with constant step is shown below.\n",
    "\n",
    "***\n",
    ">1. Give the initial value: $x_0$;\n",
    ">2. Randomly choose term index $k_{i}$ in the $k$th iteration from ${1,2,\\cdots, n}$;\n",
    ">3. Update the parameters: $${\\theta}_{k+1}={\\theta}_{k} - \\alpha g_{k_{i}}({\\theta}_{k})$$.\n",
    "***\n",
    "*Note the difference with Gradient Descent: the index $k_{i}$ is a random variable!\n",
    "*We can randomly choose a minibatch of training set to compute the gradient instead of the single sample:${\\theta}_{k+1}={\\theta}_{k} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}g_{k_{i}}({\\theta}_{k})$.\n",
    "\n",
    "\n",
    "\n",
    "[See also at Wikipedia.](https://www.wikiwand.com/en/Stochastic_gradient_descent) and [SGD in CS231](https://cs231n.github.io/neural-networks-3/#sgd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation \n",
    "\n",
    "[Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/abs/1606.04838) focus on the finite sum cost function with extremly large sample szie $n$.\n",
    "It is inspired by statistics that the population can be estimated by sampling. \n",
    "\n",
    "There are more and more courses in dig data algorithms which concerns on the gradient methods. The following is a partial list on gradient methods:\n",
    "* [The course in PKU](http://bicmr.pku.edu.cn/~wenzw/bigdata2017.html);\n",
    "* [Large-scale Machine Learning and Optimization](http://papail.io/901);\n",
    "* [A blog on optimization in machine learning](http://freemind.pluskid.org/series/mlopt/).\n",
    "\n",
    "## Why SGD ?\n",
    "In deep learning, we always need tremendrous training set to fit the model with large size.\n",
    "It is costly to compute the gradient of empirical risk function exactly owing to the large term number/sample size- n.\n",
    "***\n",
    "\n",
    ">1. SGD is independent of the term number in each iteration so that it is cheap to compute.\n",
    ">2. In [this post](http://www.offconvex.org/2017/07/19/saddle-efficiency/), SGD also can escape the saddle point.\n",
    ">3. It shows that gradient descent with random initialization converges to local minimizer in [that paper](https://arxiv.org/abs/1602.04915).\n",
    ">4. [Continuous-Time Limit](http://www.stephanmandt.com/papers/Mandtetal_NIPS_OPT2015.pdf) of SGD may bring some new ideas.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive learning rate\n",
    "\n",
    "**Note that the $\\alpha$ is key in the convergence even in the convex case.**\n",
    "There is a wonderful [overview](http://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "There are some strategies of choosing the step sizes.\n",
    "### Adagrad\n",
    "\n",
    "It is based on the experience that infrequent parameters can give more information than the frequent parameters.\n",
    "Thus it is designed to perform larger updates for infrequent and smaller updates for frequent parameters. \n",
    "\n",
    "It individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square\n",
    "root of the sum of all the historical squared values of the gradient.\n",
    "\n",
    "***\n",
    "1. Sample a minibatch of m examples from the training set $\\{x_{k_1}, x_{k_2}, \\dots, x_{k_m}\\}$ with corresponding targets $y_{k_i}$;\n",
    "2. Compute the gradinet:$g_{k} = \\frac{1}{m}\\sum_{i=1}^{m}g_{k_{i}}({\\theta})$;\n",
    "3. Accumulate squared gradient: $r_{k+1}=r_k+g_{k+1}\\odot g_{k+1}$, where $\\odot$ is multiplication applied element-wise;\n",
    "4. Update: ${\\theta}_{k+1}={\\theta}_k - \\frac{\\varepsilon}{\\delta + \\sqrt{r_{k+1}}}\\odot g_k$ (Division and square root applied element-wise).\n",
    "***\n",
    "But why is it the sum of **all** the historical squared values of the gradinet?\n",
    "It is necessary to consider the correlation between the parameters.\n",
    "### RMSprop\n",
    "RMSProp uses an **exponentially decaying average** to discard history from the extreme past.\n",
    "\n",
    "***\n",
    "1. Sample a minibatch of m examples from the training set $\\{x_{k_1}, x_{k_2}, \\dots, x_{k_m}\\}$ with corresponding targets $y_{k_i}$;\n",
    "2. Compute the gradinet:$g_{k} = \\frac{1}{m}\\sum_{i=1}^{m}g_{k_{i}}({\\theta})$;\n",
    "3. Accumulate squared gradient: $r_{k+1}= \\rho r_k+(1-\\rho)g_{k+1}\\odot g_{k+1}$, where $\\odot$ is multiplication applied element-wise;\n",
    "4. Update: ${\\theta}_{k+1}={\\theta}_k - \\frac{\\varepsilon}{\\delta + \\sqrt{r_{k+1}}}\\odot g_k$ (Division and square root applied element-wise).\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter.\n",
    "It can be seen as a variant on the combination of RMSProp and momentum.\n",
    "***\n",
    "1. Sample a minibatch of m examples from the training set $\\{x_{k_1}, x_{k_2}, \\dots, x_{k_m}\\}$ with corresponding targets $y_{k_i}$;\n",
    "2. Compute the gradinet:$g_{k} = \\frac{1}{m}\\sum_{i=1}^{m}g_{k_{i}}({\\theta})$;\n",
    "3. Update biased first moment estimate: $s_{k+1}={\\rho}_1 s_k + (1-{\\rho}_1)g_k$;\n",
    "4. Update biased second moment estimate: $r_{k+1}={\\rho}_2 r_k + (1-{\\rho}_2)g_k\\odot g_k$;\n",
    "5. Correct bias in first moment: $\\hat s_{k+1}=\\frac{s_{k+1}}{1-{\\rho}_{1}^k}$;\n",
    "6. Correct bias in second moment: $\\hat r_{k+1}=\\frac{r_{k+1}}{1-{\\rho}_{2}^k}$;\n",
    "6. Update: ${\\theta}_{k+1}={\\theta}_k -\\varepsilon \\frac{\\hat s_{k+1}}{\\sqrt{\\hat r_{k+1}}+\\delta}$  .\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance reduction \n",
    "\n",
    "The stochastic gradient descnet is not always descent due to its inherent variance. There is [fluctuations in the total objective function as gradient steps with respect to mini-batches are taken](https://www.wikiwand.com/en/Stochastic_gradient_descent).\n",
    "![wikimedia](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Stogra.png/440px-Stogra.png).\n",
    "\n",
    "\n",
    "We want faster algorithm by incorporating more information on the cost function.\n",
    "[Accelerating Stochastic Gradient Descent](https://arxiv.org/abs/1704.08227v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Average  Gradient (SAG)\n",
    "Like stochastic gradient (SG) methods, the SAG method’s iteration cost is independent of the number of terms in the sum.\n",
    "However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods in the [introduction](https://arxiv.org/abs/1309.2388). \n",
    "\n",
    "The SAG iterations take the form:\n",
    "$${\\theta}_{k+1}={\\theta}_k - \\frac{{\\alpha}_k }{n}\\sum _{i=1}^{n}y_{i_k} ,$$\n",
    "where at each iteration a random index $i_k$ is selected and we set:\n",
    "$$y_{i_k}=\\begin{cases}\n",
    "g_{i}({\\theta}_k),  & \\text{if $i=i_k$} \\\\\n",
    "y_{i_{k-1}}, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVRG\n",
    "[Rie Johnson and Tong Zhang](http://riejohnson.com/rie/stograd_nips.pdf)  introduce an explicit variance reduction method for stochastic gradient descent. It is proved that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG) for *smooth and strongly convex* functions. \n",
    "***\n",
    "1. During the $s$th epoch, $\\hat{\\theta}={\\hat{\\theta}}_{s-1}$, $\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{m} g_i (\\hat{\\theta})$,${\\theta}_0=\\hat{\\theta}$;\n",
    "2. For $k\\in \\{ 1, 2, \\cdots, m\\}$\n",
    "\n",
    "  Randomly pick $i_k \\in \\{1,2,\\cdots, n\\}$ and update weight:\n",
    " $${\\theta}_k={\\theta}_{k-1}-\\alpha(g_{i_k}({\\theta}_{k-1})-g_{i_k}(\\hat{\\theta})+\\hat{\\mu})$$.\n",
    " \n",
    "3. ${\\hat{\\theta}}_{s}={\\theta}_m$ or ${\\hat{\\theta}}_{s}= {\\theta}_k $ for randomly chosen $k \\in \\{1,2,\\cdots, m-1\\} $\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVRG++\n",
    "\n",
    "[It is improved SVRG for non-strongly convex or sum-ofnon-convex settings.](https://arxiv.org/abs/1506.01972).\n",
    "The objective function is $$F(x)=\\sum _{i=1}^{n}f_{i}(x)+h(x)$$, where $h(x)$ is the regularizion term.\n",
    "Let $g_i(x)={\\nabla}_{x} f_i(x)$\n",
    "***\n",
    "1. During the $s$th epoch, $\\hat{\\mu}_{s-1}=\\frac{1}{m_{s-1}}\\sum_{i=1}^{m} g_i ({\\hat{x}}_{s-1})$;\n",
    "2. $m_s=2^s m_{0}$ \n",
    "3. For $k\\in \\{ 0,1, 2, \\cdots, m_s -1\\}$\n",
    "\n",
    " Randomly pick $i_k \\in \\{1,2,\\cdots, n\\}$ and update weight:\n",
    " $$ \\hat{w}=g_{i_k}({x}_{k}^s)-g_{i_k}(\\hat{x}_{s-1})+\\hat{\\mu}_s$$,\n",
    " $$x_{k+1}^s=argmin_{x}\\{ h(x)+\\frac{1}{2\\gamma}\\| x- x_k^s\\|\\}+<x,\\hat{w}>\\}$$\n",
    " \n",
    "4. ${\\hat x}_{s}=\\frac{1}{m_s}\\sum _{k=1}^{m_s}x_{k}^s$ ;\n",
    "5. $x_0 ^{s+1}=x_{m_s}^s$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA\n",
    "\n",
    "[SAGA](http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf) improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support\n",
    "for composite objectives where a proximal operator is used on the regularizer.\n",
    "The objective function is $$F(x)=\\sum _{i=1}^{n}f_{i}(x)+h(x),$$ where $h(x)$ is the regularizion term.\n",
    "Let $g_i(x)={\\nabla}_{x} f_i(x)$\n",
    "***\n",
    "Given the value of $x_k$ and of each $g_i({\\phi}_{i_k})$ at the end of iteration $k$, the updates\n",
    "for iteration $k + 1$ is as follows:\n",
    "1. Pick a $j$ uniformly at random. \n",
    "2. Take ${\\phi}_{j_{k+1}}= x_k$, store $g_j({\\phi}_{j_{k+1}})$ in the table and other entries in the table remain unchanged. \n",
    "3. Updates in 2 steps: \n",
    "$$w_{k+1}= x_k - \\alpha(g_j({\\phi}_{j_{k+1}})-g_j({\\phi}_{j_{k}})+\\frac{1}{n}\\sum_{i=1}^{n}g_j({\\phi}_{j_{k}}))$$\n",
    "$$x_{k+1}=argmin_{x}\\{ h(x)+\\frac{1}{2\\gamma}\\| x- w_{k+1}\\|\\}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCSG\n",
    "[Less than a Single Pass](https://arxiv.org/abs/1609.03261).\n",
    "As a member of the SVRG family of algorithms, \n",
    "SCSG makes use of gradient estimates at two scales, \n",
    "with the number of updates at the faster scale being governed by a geometric random variable. \n",
    "Unlike most existing algorithms in this family, both the computation cost\n",
    "and the communication cost of SCSG do not necessarily scale linearly with the\n",
    "sample size $n$; indeed, these costs are independent of $n$ when the target accuracy is low. \n",
    "***\n",
    "1. During the *$\\color{red}s$*th epoch and $s\\in \\{ 1, 2, \\cdots, T\\}$, $\\hat{\\theta}={\\hat{\\theta}}_{s-1}$, $\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{m} g_i (\\hat{\\theta})$,${\\theta}_0=\\hat{\\theta}$;\n",
    "2. For $k\\in \\{ 1, 2, \\cdots, M\\}$, where $M \\sim Geom(\\frac{B}{B+1})$ \n",
    "\n",
    "  Randomly pick $i_k \\in \\{1,2,\\cdots, n\\}$ and update weight:\n",
    " $${\\theta}_k={\\theta}_{k-1}-\\alpha(g_{i_k}({\\theta}_{k-1})-g_{i_k}(\\hat{\\theta})+\\hat{\\mu})$$.\n",
    " \n",
    "3. In this epoch:${\\hat{\\theta}}_{s}={\\theta}_M$\n",
    " \n",
    "4. (Strongly convex case)${\\hat{\\theta}}={\\theta}_T$ or (Not strongly convex case)${\\hat{\\theta}}= \\frac{1}{T}\\sum _{k=1}^{T}{\\theta}_k $\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: the *3* directions in SGD\n",
    "\n",
    "### How to choose the steps?\n",
    "There is a wonderful [overview](http://ruder.io/optimizing-gradient-descent/).\n",
    "### How to reduce the variance of SGD?\n",
    "The answer is SVRG or stochastic variance reduction gradient algoithm by introducing some variance reducing techniques in statistics to optimization by [Lin Xiao and Tong Zhang](https://arxiv.org/abs/1403.4699). And he did [muck work](http://tongzhang-ml.org/publication.html) on this.\n",
    "\n",
    "### How to accelerate SGD?\n",
    "By incorporating momentum as shown in the last overview or adaptive steps.\n",
    "[Accelerating Stochastic Gradient Descent](https://arxiv.org/abs/1704.08227v1) and [This paper](https://arxiv.org/pdf/1603.04245.pdf) shows that there is a Lagrangian functional that we call the Bregman Lagrangian which generates a large class of accelerated methods in continuous time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Surface of Deep Learning\n",
    "The latest paper (not read):\n",
    "***\n",
    "1. [The Loss Surface of Deep and Wide Neural Networks](https://arxiv.org/pdf/1704.08045.pdf);\n",
    "2. [Global optimality conditions for deep neural networks](https://arxiv.org/abs/1707.02444);\n",
    "3. [Deep Learning without Poor Local Minima](https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf);\n",
    "4. [Empirical Risk for Non-convex Losses](https://arxiv.org/abs/1607.06534);\n",
    "5. [Convergence of Neural Network with ReLU](https://arxiv.org/abs/1611.02345);\n",
    "6. [On Large-Batch Training](https://arxiv.org/abs/1609.04836)\n",
    "7. [This paper provides the first convergence guarantee applicable to modern convnets](https://openreview.net/pdf?id=HyWG0H5ge)\n",
    "8. [Entropy-SGD: Biasing Gradient Descent Into Wide Valleys](https://arxiv.org/abs/1611.01838)\n",
    "***\n",
    "\n",
    "**Always remeber the big picture and do not forget the regularization or generalization.**\n",
    "\n",
    "1. [Theory of deep learning](https://sites.google.com/site/deeplearningtheory/schedule);\n",
    "2. [Principled Approaches to Deep Learning](http://padl.ws/)\n",
    "3. [An exact mapping between the Variational Renormalization Group and Deep Learning](https://arxiv.org/pdf/1410.3831.pdf).\n",
    "4. [Machine Learning, Renormalization Group and Phase Transition](http://guava.physics.uiuc.edu/~nigel/courses/563/Essays_2017/PDF/Luo.pdf).\n",
    "5. [Unreasonable Effectiveness of Learning Neural Networks](https://arxiv.org/pdf/1605.06444.pdf).\n",
    "6. [DEEP RELAXATION: PARTIAL DIFFERENTIAL EQUATIONS FOR OPTIMIZING DEEP NEURAL NETWORKS](https://arxiv.org/pdf/1704.04932.pdf).\n",
    "7.[Deep Learning theory](https://stats385.github.io/readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Deep Learning Problem\n",
    "The deep models are diffcult to train because the gradients vanish or explore.\n",
    "BP may not the optimal choice.\n",
    "LSTM is invented to solve this problem by constructing some \"memory\" cells.\n",
    "ResNet adds more connetions to solve the degradation.\n",
    "\n",
    "But SGD cannot guarantee the convergence to global optima in nonconvex case.\n",
    "\n",
    "* [Failures of Gradient-Based Deep Learning](http://proceedings.mlr.press/v70/shalev-shwartz17a/shalev-shwartz17a.pdf)\n",
    "\n",
    "* [The Shattered Gradients Problem: If resnets are the answer, then what is the question?](https://arxiv.org/abs/1702.08591)\n",
    "and [more paper on this problem](http://www.arxiv-sanity.com/1702.08591).\n",
    "\n",
    "* [Gradient flow in recurrent nets](http://www.bioinf.jku.at/publications/older/ch7.pdf)cited as Hochreiter S, Bengio Y, Frasconi P, et al.Gradient flow in recurrent nets: the difficulty of learning long-term dependencies[J]. 2001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization = Sparity?\n",
    "The regularization term as imbedded feature selection can make the optimization more complex such as [$\\ell_1$](https://arxiv.org/abs/1510.03528).\n",
    "\n",
    "\n",
    "ADMM or SDCA?\n",
    "\n",
    "The ultimate aim is to decrease the generalization error by designing some regularization. \n",
    "There are some explaination for some regularization such as the sparsity of $\\ell_1$.\n",
    "But there are some doubts on the sparity in artificial neural network. \n",
    "It is still mysterious to understand the generalization error except the cross validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nonconvex Optimization\n",
    "\n",
    "For noconvex case, we are to find the *stationary point* , that is the point with small gradient.\n",
    "\n",
    "**Local Holder continuous function, convex difference function**\n",
    "\n",
    "### Convexify\n",
    "If we can transform the nonconvex functions to convex functions, the nonconvex optimization can be been solved by convex optimization.\n",
    "[In one and two dimensions](https://www.researchgate.net/publication/244958442_A_method_to_convexify_functions_via_curve_evolution), it is done.\n",
    "CCCP or MM approxiamated the  cost function by Taylor expansion.\n",
    "\n",
    "### Convex splitting\n",
    "Difference convex programmming or DC programming is diffferent from the SGD but related to MM algorithm in computational statistics.\n",
    "\n",
    "### Smoothing\n",
    "If the cost function is not convex, we would like to optimize a convex function related with it such as the proximal gradient descent and mirror gradient descent.\n",
    "Or beyond the Euclidean distance.\n",
    "[Smoothing methods for nonsmooth, nonconvex minimization](https://link.springer.com/article/10.1007%2Fs10107-012-0569-0?LI=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement it?\n",
    "Tensorflow bulit many optimizer including many SGD.\n",
    "My thesis is to clarify the problem and create a new alorithm to solve the training procedure of deep learning models.\n",
    "It is only about 10 days.\n",
    "\n",
    ">Talk is cheap and show me the code.\n",
    "\n",
    "Programming is necessary and the application is the core.\n",
    "\n",
    "Our main aims:\n",
    "1. Undersatnd SGD and its variants.\n",
    "2. Find their advantages and disadvantages by experiment.\n",
    "3. Make the programs run faster by changing some steps.\n",
    "\n",
    "**Just do it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read the code in TensorFlow\n",
    "***\n",
    "1. Backprobagation: how to compute the gradinet of the mulitlayer composition function.\n",
    "2. How to update the parameters.\n",
    "3. How to choose the index at random.\n",
    "***\n",
    "\n",
    "***\n",
    "> Good begin is half of success.\n",
    "> DO NOT QUIT.\n",
    "> Keep up.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mirror, natural or zero gradient?\n",
    "Write down the corresponding algoirthm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Noise, Perturbation and Initialization\n",
    "How to make it efficent to converge to the optima?\n",
    "Some empirical works on the noise and initialization such as \n",
    "* [On the importance of initialization and momentum in deep learning](http://proceedings.mlr.press/v28/sutskever13.html),\n",
    "* [Adding Gradient Noise Improves Learning for Very Deep Networks](https://arxiv.org/abs/1511.06807).\n",
    "* [ORF523: Noisy oracles](https://blogs.princeton.edu/imabandit/2013/04/25/orf523-noisy-oracles/)\n",
    "* [On the role of synaptic stochasticity in training low-precision neural networks](https://arxiv.org/abs/1710.09825v1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "299px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "503px",
    "left": "0px",
    "right": "1154px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
